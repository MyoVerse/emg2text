{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnn\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof for table 1, figure 3, and figure 4. Icefall code is from https://github.com/k2-fsa/icefall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train LARGE-VOCAB EMG-to-phoneme conversion.\n",
    "\n",
    "For description of the data, please see largeVocabDataVisualization.ipynb\n",
    "\n",
    "Given a sentence, you decode it fully using CTC loss. The pipeline resembles standard speech-to-text (ASR) techniques.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"https://pypi.org/project/Levenshtein/ - install this Lev distance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Open Data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(\"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/dataLargeVocab.pkl\", \"rb\") as file:\n",
    "    DATA = pickle.load(file)\n",
    "\n",
    "with open(\"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/labelsLargeVocab.pkl\", \"rb\") as file:\n",
    "    LABELS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diag = TRUE or FALSE. Raw SPD matrices or approximately diagonalized?\"\"\"\n",
    "DIAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "English phoneme definitions.\n",
    "\"\"\"\n",
    "tok2id = {}\n",
    "with open(\"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/ckptsLargeVocab/lang_phone/tokens.txt\") as f:\n",
    "    for line in f:\n",
    "        s, i = line.strip().split()\n",
    "        i = int(i)\n",
    "        if s == \"<eps>\" or s.startswith(\"#\"):\n",
    "            continue\n",
    "        tok2id[s] = i\n",
    "PHONE_DEF = tok2id\n",
    "\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF[p]\n",
    "\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phonemize the sentences.\n",
    "\"\"\"\n",
    "\n",
    "phonemizedSentences = []\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    phones = []\n",
    "    for p in g2p(LABELS[i]): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p): \n",
    "            phones.append(p)\n",
    "    phonemizedSentences.append(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert phone-to-indices using look-up dictionary PHONE_DEF.\n",
    "\"\"\"\n",
    "\n",
    "phoneIndexedSentences = []\n",
    "for i in range(len(phonemizedSentences)):\n",
    "    current = phonemizedSentences[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phoneIndexedSentences.append(phoneID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenIdToClassIdx(tokenId: int) -> int:\n",
    "    return tokenId - 1   \n",
    "\n",
    "def phoneSeqToClassIdxSeq(phoneSeq):\n",
    "    return [tokenIdToClassIdx(PHONE_DEF[p]) for p in phoneSeq]\n",
    "\n",
    "classIndexedSentences = [phoneSeqToClassIdxSeq(seq) for seq in phonemizedSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pad the phone transcribed sentences to a common length (to be used with CTC loss).\n",
    "\"\"\"\n",
    "\n",
    "phonemizedLabels = np.zeros((len(classIndexedSentences), 76)) - 1\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    phonemizedLabels[i, 0:len(classIndexedSentences[i])] = classIndexedSentences[i]\n",
    "\n",
    "labelLengths = np.zeros((len(classIndexedSentences)))\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    labelLengths[i] = len(classIndexedSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z-normalize the data along the time dimension.\n",
    "\"\"\"\n",
    "\n",
    "normDATA = []\n",
    "for i in range(len(DATA)):\n",
    "    Mean = np.mean(DATA[i], axis = -1)\n",
    "    Std = np.std(DATA[i], axis = -1)\n",
    "    normDATA.append((DATA[i] - Mean[..., np.newaxis])/Std[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Slice the matrices into 50ms segments with a step size of 20ms. Signal is sampled at 5000 Hertz.\n",
    "\"\"\"\n",
    "\n",
    "slicedMatrices = []\n",
    "for j in range(len(normDATA)):\n",
    "    collect = []\n",
    "    stepSize = 100 \n",
    "    windowSize = 125\n",
    "    dataLength = normDATA[j].shape[1]\n",
    "    numIters = (dataLength - windowSize) // stepSize + 1\n",
    "       \n",
    "    for i in range(numIters):\n",
    "        where = i * stepSize + windowSize\n",
    "        start = where - windowSize\n",
    "        End = where + windowSize\n",
    "        temp = 1/(2 * windowSize) * (normDATA[j][:, start:End] @ normDATA[j][:, start:End].T)\n",
    "        collect.append(0.9 * temp + 0.1 * np.trace(temp) * np.eye(31))\n",
    "    slicedMatrices.append(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Approximately diagonalize the matrices using Frechet mean. Use only TRAIN-VAL data for calculating Frechet mean.\n",
    "\"\"\"\n",
    "\n",
    "matricesForMean = []\n",
    "for i in range(9000):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        matricesForMean.append(slicedMatrices[i][j])\n",
    "\n",
    "matricesForMean = np.array(matricesForMean)\n",
    "manifoldMean = frechetMean()\n",
    "\n",
    "MEAN = manifoldMean.mean(matricesForMean.reshape(-1, 31, 31))\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "identityMatrix = np.eye(31)\n",
    "afterMatrices = np.tile(identityMatrix, (len(slicedMatrices), 409, 1, 1)) \n",
    "inputLengths = np.zeros((len(slicedMatrices)))\n",
    "for i in range(len(slicedMatrices)):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        if DIAG:\n",
    "            temp = eigenvectors.T @ slicedMatrices[i][j] @ eigenvectors\n",
    "        else:\n",
    "            temp = slicedMatrices[i][j]\n",
    "        afterMatrices[i, j] = temp\n",
    "    inputLengths[i] = len(slicedMatrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"np.save(\"DATA/ckptsLargeVocab/frechetMeanLargeVocab.npy\", MEAN)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, inputLength, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "        self.inputLength = inputLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.inputLength[index])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train-validation-test split.\n",
    "\"\"\"\n",
    "trainFeatures = afterMatrices[:8000]\n",
    "trainLabels = phonemizedLabels[:8000]\n",
    "trainLabelLengths = labelLengths[:8000]\n",
    "trainInputLengths = inputLengths[:8000]\n",
    "\n",
    "valFeatures = afterMatrices[8000:9000]\n",
    "valLabels = phonemizedLabels[8000:9000]\n",
    "valLabelLengths = labelLengths[8000:9000]\n",
    "valInputLengths = inputLengths[8000:9000]\n",
    "\n",
    "testFeatures = afterMatrices[9000:]\n",
    "testLabels = phonemizedLabels[9000:]\n",
    "testLabelLengths = labelLengths[9000:]\n",
    "testInputLengths = inputLengths[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainInputLengths, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valInputLengths, valLabelLengths)\n",
    "testDataset = BaseDataset(testFeatures, testLabels, testInputLengths, testLabelLengths)\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)\n",
    "testDataloader = DataLoader(testDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOperation(model,  device, trainLoader, rnnOptimizer, Loss):\n",
    "    model.train()\n",
    "    totalLoss = 0\n",
    "    for inputs, targets, inputLengths, targetLengths in trainLoader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "        \n",
    "        rnnOptimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, inputLengths.cpu())\n",
    "        loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "        loss.backward()\n",
    "        rnnOptimizer.step()\n",
    "\n",
    "        totalLoss += loss.item()\n",
    "        \n",
    "    \n",
    "    return totalLoss / len(trainLoader)\n",
    "\n",
    "\n",
    "def valOperation(model, device, valLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in valLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "\n",
    "    return totalLoss / len(valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348591\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To replicate the PER (phoneme error rate) for various model sizes and layers, change the variable here:\n",
    "euclideanRnn.RnnNet(41, modelHiddenDimension = 25, device, numLayers = 3).to(device)\n",
    "\"\"\"\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "model = euclideanRnn.RnnNet(41, 25, device, numLayers = 3).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 40, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do training.\n",
    "\"\"\"\n",
    "\n",
    "valLOSS = []\n",
    "minLOSS = 100\n",
    "for epoch in range(numberEpochs):\n",
    "    trainLoss = trainOperation(model, device, trainDataloader, rnnOptimizer, lossFunction)\n",
    "    valLoss = valOperation(model, device, valDataloader, lossFunction)\n",
    "    valLOSS.append(valLoss)\n",
    "    if minLOSS > valLoss:\n",
    "        minLOSS = valLoss\n",
    "    torch.save(model.state_dict(), \"ckpts/largeVocab/\" + str(epoch) + \".pt\")\n",
    "    print(f'Epoch: {epoch + 1}/{numberEpochs}, Training loss: {trainLoss:.4f}, Val loss: {valLoss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ckpts/largeVocab/valLoss.npy\", valLOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6644263043999672\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "valLoss = np.load(\"ckpts/largeVocab/valLoss.npy\")\n",
    "print(np.min(valLoss))\n",
    "print(np.argmin(valLoss))\n",
    "epoch = np.argmin(valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOperation(model, device, testLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    Outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in testLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "\n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "            Outputs.append(outputs.transpose(0, 1))\n",
    "\n",
    "    return Outputs, totalLoss / len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple beam-search algorithm.\n",
    "\"\"\"\n",
    "\n",
    "def ctcPrefixBeamSearch(\n",
    "    logProbs,\n",
    "    testLen = None,\n",
    "    beamSize = 5,\n",
    "    blank = 40,\n",
    "    topk = None,\n",
    "    allowDoubles = True,\n",
    "):\n",
    "    \n",
    "    lp = np.asarray(logProbs)\n",
    "    Ttotal, V = lp.shape\n",
    "    T = Ttotal if testLen is None else int(min(testLen, Ttotal))\n",
    "\n",
    "    beams = {(): (0.0, -np.inf)}\n",
    "\n",
    "    def add(store, seq, addPb, addPnb):\n",
    "        if seq in store:\n",
    "            pb, pnb = store[seq]\n",
    "            if addPb  != -np.inf: pb  = np.logaddexp(pb,  addPb)\n",
    "            if addPnb != -np.inf: pnb = np.logaddexp(pnb, addPnb)\n",
    "            store[seq] = (pb, pnb)\n",
    "        else:\n",
    "            store[seq] = (addPb, addPnb)\n",
    "\n",
    "    for t in range(T):\n",
    "        row = lp[t] \n",
    "        new = {}\n",
    "\n",
    "        if topk is not None and topk < V:\n",
    "            cand = np.argpartition(row, -topk)[-topk:]\n",
    "            if blank not in cand:\n",
    "                worstIdx = cand[np.argmin(row[cand])]\n",
    "                cand[cand == worstIdx] = blank\n",
    "        else:\n",
    "            cand = range(V)\n",
    "\n",
    "        for seq, (pb, pnb) in beams.items():\n",
    "            add(new, seq, np.logaddexp(pb, pnb) + row[blank], -np.inf)\n",
    "\n",
    "            last = seq[-1] if seq else None\n",
    "\n",
    "            for c in cand:\n",
    "                if c == blank:\n",
    "                    continue\n",
    "                pC = row[c]\n",
    "\n",
    "                if c == last:\n",
    "            \n",
    "                    add(new, seq, -np.inf, pnb + pC)\n",
    "\n",
    "                    if allowDoubles:\n",
    "                        add(new, seq + (c,), -np.inf, pb + pC)\n",
    "                else:\n",
    "                    add(new, seq + (c,), -np.inf, np.logaddexp(pb, pnb) + pC)\n",
    "\n",
    "        if len(new) > beamSize:\n",
    "            items = sorted(new.items(),\n",
    "                           key = lambda kv: np.logaddexp(*kv[1]),\n",
    "                           reverse = True)[:beamSize]\n",
    "            beams = dict(items)\n",
    "        else:\n",
    "            beams = new\n",
    "\n",
    "    bestSeq = max(beams.items(), key = lambda kv: np.logaddexp(*kv[1]))[0]\n",
    "    return bestSeq\n",
    "\n",
    "def findClosestTranscription(decodedTranscript, phoneticTranscription):\n",
    "    \n",
    "    dist = Levenshtein.distance(decodedTranscript, phoneticTranscription)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS:  1.942983986869935\n"
     ]
    }
   ],
   "source": [
    "modelWeight = torch.load(\"ckpts/largeVocab/\" + str(epoch)  + '.pt', weights_only = True)\n",
    "model.load_state_dict(modelWeight)\n",
    "output, testLoss = testOperation(model, device, testDataloader, lossFunction)\n",
    "\n",
    "print(\"TEST LOSS: \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for o in output:\n",
    "    for oo in o:\n",
    "        outs.append(oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970\n",
      "torch.Size([192, 41])\n"
     ]
    }
   ],
   "source": [
    "print(len(outs))\n",
    "print(outs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONE_DEF1 = {}\n",
    "for k, v in PHONE_DEF.items():\n",
    "    PHONE_DEF1[v - 1] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVS = []\n",
    "decodedOut = []\n",
    "for i in range(1660):\n",
    "    decodedSymbols = ctcPrefixBeamSearch(outs[i].cpu().numpy(), testInputLengths[i]) \n",
    "    phoneOut = []\n",
    "    for i in range(len(decodedSymbols)):\n",
    "        phoneOut.append(PHONE_DEF1[decodedSymbols[i]])\n",
    "    decodedOut.append(phoneOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "levs = []\n",
    "phoneLENGTHS = []\n",
    "for i in range(len(decodedOut)):\n",
    "    phoneLENGTHS.append(len(phonemizedSentences[9000 + i]))\n",
    "    levs.append(findClosestTranscription(decodedOut[i], phonemizedSentences[9000 + i]))\n",
    "LEVS.append(np.mean(levs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sentences:  19.64096385542169\n",
      "Mean phoneme errors (insertion errors + deletion errors + substitution errors):  10.042168674698795\n",
      "Percent phoneme error:  0.5112869586553797\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of sentences: \", np.mean(phoneLENGTHS))\n",
    "print(\"Mean phoneme errors (insertion errors + deletion errors + substitution errors): \", np.mean(levs))\n",
    "print(\"Percent phoneme error: \", np.sum(levs)/np.sum(phoneLENGTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 991 1346  452 1056  682  989  214 1476 1619 1478   59  170  997  564\n",
      "  835 1179 1381  939  687 1373 1222 1007 1504 1157 1467  852 1326 1214\n",
      " 1035  839 1623  784  536  173 1538 1491 1341  995  973  621  620  633\n",
      "   34  983  850  520   90  985  741    8 1121   48  215  594 1072  540\n",
      "  627  134  120 1185 1315 1620   35  690  669 1318  655  392  815 1542\n",
      " 1135  674  759  362  108 1581 1447  821 1216  169 1543  797 1518 1646\n",
      "  703  988  496 1436  302  478 1649 1332  602  673  650  638  313   80\n",
      "  738  539]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sort the decoded sentences from best-to-worst. Display 100 best decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['AE', 'M', 'AH', 'Y', 'UW', 'AH', 'R', 'IY']\n",
      "Ground truth phoneme sequence:  ['W', 'EH', 'L', 'P', 'R', 'AA', 'B', 'AH', 'B', 'L', 'IY', 'B', 'IY', 'T', 'AO', 'K', 'IH', 'NG', 'T', 'UW', 'Y', 'UW', 'AH', 'G', 'EH', 'N']\n",
      "Ground truth label:  well probably be talking to you again\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  22\n",
      "Length of ground truth sequence:  26\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "which = 406\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[8000 + which])\n",
    "print(\"Ground truth label: \", LABELS[8000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[8000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[8000 + which]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
