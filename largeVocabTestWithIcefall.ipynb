{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnn\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof for table 1, figure 2, and figure 3. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test LARGE-VOCAB EMG-to-phoneme conversion.\n",
    "\n",
    "For description of the data, please see largeVocabDataVisualization.ipynb\n",
    "\n",
    "Given a sentence, you decode it fully using CTC loss. The pipeline resembles standard speech-to-text (ASR) techniques.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"https://pypi.org/project/Levenshtein/ - install this Lev distance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Open Data.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/dataLargeVocab.pkl\", \"rb\") as file:\n",
    "    DATA = pickle.load(file)\n",
    "\n",
    "with open(\"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/labelsLargeVocab.pkl\", \"rb\") as file:\n",
    "    LABELS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(LABELS)):\n",
    "    print(LABELS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "English phoneme definitions.\n",
    "\"\"\"\n",
    "tok2id = {}\n",
    "with open(\"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/ckptsLargeVocab/lang_phone/tokens.txt\") as f:\n",
    "    for line in f:\n",
    "        s, i = line.strip().split()\n",
    "        i = int(i)\n",
    "        if s == \"<eps>\" or s.startswith(\"#\"):\n",
    "            continue\n",
    "        tok2id[s] = i\n",
    "PHONE_DEF = tok2id\n",
    "\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF[p]\n",
    "\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phonemize the sentences.\n",
    "\"\"\"\n",
    "\n",
    "phonemizedSentences = []\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    phones = []\n",
    "    for p in g2p(LABELS[i]): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p): \n",
    "            phones.append(p)\n",
    "    phonemizedSentences.append(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert phone-to-indices using look-up dictionary PHONE_DEF.\n",
    "\"\"\"\n",
    "\n",
    "phoneIndexedSentences = []\n",
    "for i in range(len(phonemizedSentences)):\n",
    "    current = phonemizedSentences[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phoneIndexedSentences.append(phoneID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenIdToClassIdx(tokenId: int) -> int:\n",
    "    return tokenId - 1   \n",
    "\n",
    "def phoneSeqToClassIdxSeq(phoneSeq):\n",
    "    return [tokenIdToClassIdx(PHONE_DEF[p]) for p in phoneSeq]\n",
    "\n",
    "classIndexedSentences = [phoneSeqToClassIdxSeq(seq) for seq in phonemizedSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pad the phone transcribed sentences to a common length (to be used with CTC loss).\n",
    "\"\"\"\n",
    "\n",
    "phonemizedLabels = np.zeros((len(classIndexedSentences), 76)) - 1\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    phonemizedLabels[i, 0:len(classIndexedSentences[i])] = classIndexedSentences[i]\n",
    "\n",
    "labelLengths = np.zeros((len(classIndexedSentences)))\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    labelLengths[i] = len(classIndexedSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z-normalize the data along the time dimension.\n",
    "\"\"\"\n",
    "\n",
    "normDATA = []\n",
    "for i in range(len(DATA)):\n",
    "    Mean = np.mean(DATA[i], axis = -1)\n",
    "    Std = np.std(DATA[i], axis = -1)\n",
    "    normDATA.append((DATA[i] - Mean[..., np.newaxis])/Std[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Slice the matrices into 50ms segments with a step size of 20ms.\n",
    "\"\"\"\n",
    "\n",
    "slicedMatrices = []\n",
    "for j in range(len(normDATA)):\n",
    "    collect = []\n",
    "    stepSize = 100 \n",
    "    windowSize = 125\n",
    "    dataLength = normDATA[j].shape[1]\n",
    "    numIters = (dataLength - windowSize) // stepSize + 1\n",
    "       \n",
    "    for i in range(numIters):\n",
    "        where = i * stepSize + windowSize\n",
    "        start = where - windowSize\n",
    "        End = where + windowSize\n",
    "        temp = 1/(2 * windowSize) * (normDATA[j][:, start:End] @ normDATA[j][:, start:End].T)\n",
    "        collect.append(0.9 * temp + 0.1 * np.trace(temp) * np.eye(31))\n",
    "    slicedMatrices.append(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diag = TRUE or FALSE. Raw SPD matrices or approximately diagonalized?\"\"\"\n",
    "DIAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matricesForMean = []\n",
    "for i in range(9000):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        matricesForMean.append(slicedMatrices[i][j])\n",
    "\n",
    "matricesForMean = np.array(matricesForMean)\n",
    "manifoldMean = frechetMean()\n",
    "\n",
    "MEAN = manifoldMean.mean(matricesForMean.reshape(-1, 31, 31))\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "identityMatrix = np.eye(31)\n",
    "afterMatrices = np.tile(identityMatrix, (len(slicedMatrices), 409, 1, 1)) \n",
    "inputLengths = np.zeros((len(slicedMatrices)))\n",
    "for i in range(len(slicedMatrices)):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        if DIAG:\n",
    "            temp = eigenvectors.T @ slicedMatrices[i][j] @ eigenvectors\n",
    "        else:\n",
    "            temp = slicedMatrices[i][j]\n",
    "        afterMatrices[i, j] = temp\n",
    "    inputLengths[i] = len(slicedMatrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, inputLength, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "        self.inputLength = inputLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.inputLength[index])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train-validation-test split.\n",
    "\"\"\"\n",
    "\n",
    "trainFeatures = afterMatrices[:8000]\n",
    "trainLabels = phonemizedLabels[:8000]\n",
    "trainLabelLengths = labelLengths[:8000]\n",
    "trainInputLengths = inputLengths[:8000]\n",
    "\n",
    "valFeatures = afterMatrices[8000:9000]\n",
    "valLabels = phonemizedLabels[8000:9000]\n",
    "valLabelLengths = labelLengths[8000:9000]\n",
    "valInputLengths = inputLengths[8000:9000]\n",
    "\n",
    "testFeatures = afterMatrices[9000:]\n",
    "testLabels = phonemizedLabels[9000:]\n",
    "testLabelLengths = labelLengths[9000:]\n",
    "testInputLengths = inputLengths[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainInputLengths, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valInputLengths, valLabelLengths)\n",
    "testDataset = BaseDataset(testFeatures, testLabels, testInputLengths, testLabelLengths)\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)\n",
    "testDataloader = DataLoader(testDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348591\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To replicate the PER (phoneme error rate) for various model sizes and layers, change the variable here:\n",
    "euclideanRnn.RnnNet(40, modelHiddenDimension = 25, device, numLayers = 3).to(device)\n",
    "\"\"\"\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "model = euclideanRnn.RnnNet(41, 25, device, numLayers = 3).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 40, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOperation(model, device, testLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    Outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in testLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "\n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "            Outputs.append(outputs.transpose(0, 1))\n",
    "\n",
    "    return Outputs, totalLoss / len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple beam-search algorithm.\n",
    "\"\"\"\n",
    "\n",
    "def ctcPrefixBeamSearch(\n",
    "    logProbs,\n",
    "    testLen = None,\n",
    "    beamSize = 5,\n",
    "    blank = 40,\n",
    "    topk = None,\n",
    "    allowDoubles = True,\n",
    "):\n",
    "    \n",
    "    lp = np.asarray(logProbs)\n",
    "    Ttotal, V = lp.shape\n",
    "    T = Ttotal if testLen is None else int(min(testLen, Ttotal))\n",
    "\n",
    "    beams = {(): (0.0, -np.inf)}\n",
    "\n",
    "    def add(store, seq, addPb, addPnb):\n",
    "        if seq in store:\n",
    "            pb, pnb = store[seq]\n",
    "            if addPb  != -np.inf: pb  = np.logaddexp(pb,  addPb)\n",
    "            if addPnb != -np.inf: pnb = np.logaddexp(pnb, addPnb)\n",
    "            store[seq] = (pb, pnb)\n",
    "        else:\n",
    "            store[seq] = (addPb, addPnb)\n",
    "\n",
    "    for t in range(T):\n",
    "        row = lp[t] \n",
    "        new = {}\n",
    "\n",
    "        if topk is not None and topk < V:\n",
    "            cand = np.argpartition(row, -topk)[-topk:]\n",
    "            if blank not in cand:\n",
    "                worstIdx = cand[np.argmin(row[cand])]\n",
    "                cand[cand == worstIdx] = blank\n",
    "        else:\n",
    "            cand = range(V)\n",
    "\n",
    "        for seq, (pb, pnb) in beams.items():\n",
    "            add(new, seq, np.logaddexp(pb, pnb) + row[blank], -np.inf)\n",
    "\n",
    "            last = seq[-1] if seq else None\n",
    "\n",
    "            for c in cand:\n",
    "                if c == blank:\n",
    "                    continue\n",
    "                pC = row[c]\n",
    "\n",
    "                if c == last:\n",
    "            \n",
    "                    add(new, seq, -np.inf, pnb + pC)\n",
    "\n",
    "                    if allowDoubles:\n",
    "                        add(new, seq + (c,), -np.inf, pb + pC)\n",
    "                else:\n",
    "                    add(new, seq + (c,), -np.inf, np.logaddexp(pb, pnb) + pC)\n",
    "\n",
    "        if len(new) > beamSize:\n",
    "            items = sorted(new.items(),\n",
    "                           key = lambda kv: np.logaddexp(*kv[1]),\n",
    "                           reverse = True)[:beamSize]\n",
    "            beams = dict(items)\n",
    "        else:\n",
    "            beams = new\n",
    "\n",
    "    bestSeq = max(beams.items(), key = lambda kv: np.logaddexp(*kv[1]))[0]\n",
    "    return bestSeq\n",
    "\n",
    "def findClosestTranscription(decodedTranscript, phoneticTranscription):\n",
    "    \n",
    "    dist = Levenshtein.distance(decodedTranscript, phoneticTranscription)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6644263043999672\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "valLoss = np.load(\"ckpts/largeVocab/valLoss.npy\")\n",
    "print(np.min(valLoss))\n",
    "print(np.argmin(valLoss))\n",
    "epoch = np.argmin(valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS:  1.942983986869935\n"
     ]
    }
   ],
   "source": [
    "modelWeight = torch.load(\"ckpts/largeVocab/\" + str(epoch)  + '.pt', weights_only = True)\n",
    "model.load_state_dict(modelWeight)\n",
    "output, testLoss = testOperation(model, device, testDataloader, lossFunction)\n",
    "\n",
    "print(\"TEST LOSS: \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for o in output:\n",
    "    for oo in o:\n",
    "        outs.append(oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONE_DEF1 = {}\n",
    "for k, v in PHONE_DEF.items():\n",
    "    PHONE_DEF1[v - 1] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVS = []\n",
    "decodedOut = []\n",
    "for i in range(1970):\n",
    "    decodedSymbols = ctcPrefixBeamSearch(outs[i].cpu().numpy(), testInputLengths[i]) \n",
    "    phoneOut = []\n",
    "    for j in range(len(decodedSymbols)):\n",
    "        phoneOut.append(PHONE_DEF1[decodedSymbols[j]])\n",
    "    decodedOut.append(phoneOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "levs = []\n",
    "phoneLENGTHS = []\n",
    "for i in range(len(decodedOut)):\n",
    "    phoneLENGTHS.append(len(phonemizedSentences[9000 + i]))\n",
    "    levs.append(findClosestTranscription(decodedOut[i], phonemizedSentences[9000 + i]))\n",
    "LEVS.append(np.mean(levs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sentences:  19.590862944162435\n",
      "Mean phoneme error rate (insertion errors + deletion errors + substitution errors):  10.06497461928934\n",
      "Percent phoneme error:  0.5137586153288076\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of sentences: \", np.mean(phoneLENGTHS))\n",
    "print(\"Mean phoneme error rate (insertion errors + deletion errors + substitution errors): \", np.mean(levs))\n",
    "print(\"Percent phoneme error: \", np.mean(levs)/np.mean(phoneLENGTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 991 1346 1756  452 1056  682  989 1619   59 1476 1478  214 1920  997\n",
      " 1751  170  564 1179 1381  835  939  687 1373 1222 1035 1007 1538 1157\n",
      "  852 1467  173  839  536 1623 1326 1504 1214  784 1491 1341 1690  995\n",
      " 1821 1807  973  621  620   34 1855  983  850  633  520   90 1732  985\n",
      " 1121  741    8   48  215  594  655 1318 1315  690  669  134  540 1620\n",
      " 1802   35  627 1185  120 1072 1771 1898  815  392 1714 1542 1135  674\n",
      "  108 1924 1581  759  362 1447  169 1543  797  821 1927 1216 1518 1800\n",
      "  703 1436]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sort the decoded sentences from best-to-worst. Display 100 best decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['DH', 'EY', 'V', 'G', 'AA', 'T', 'AH', 'N', 'AY', 'S', 'W', 'AH', 'N']\n",
      "Ground truth phoneme sequence:  ['DH', 'EY', 'V', 'G', 'AA', 'T', 'AH', 'N', 'AY', 'S', 'W', 'AH', 'N']\n",
      "Ground truth label:  theyve got a nice one\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  0\n",
      "Length of ground truth sequence:  13\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "which = 991\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[9000 + which])\n",
    "print(\"Ground truth label: \", LABELS[9000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[9000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[9000 + which]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ICEFALL_ROOT = Path(\"/mnt/dataDrive/emgFullCorpora/toUpload/Icefall/\").resolve()\n",
    "if str(ICEFALL_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ICEFALL_ROOT))\n",
    "\n",
    "\n",
    "from icefall.lexicon import Lexicon\n",
    "from icefall.decode import get_lattice, one_best_decoding\n",
    "from icefall.utils import get_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HLG is created using the following steps.\n",
    "We use LibriSpeech-100 ranscriptions to create the graph. It has approximalety 38000 sentences and 35000 unique words. \n",
    "\n",
    "1) python prepare_lang.py --lang-dir DATA/ckptsLargeVocab/lang_phone --debug false (this expects lexicon.txt)\n",
    "\n",
    "2a) /mnt/dataDrive/kenLM/kenlm/build/bin/lmplz -o 4   < DATA/ckptsLargeVocab/libri100.txt > DATA/ckptsLargeVocab/lm/4gram.arpa\n",
    " b)  python3 -m kaldilm   --read-symbol-table DATA/ckptsLargeVocab/lang_phone/words.txt   --disambig-symbol '#0' --max-order 4   DATA/ckptsLargeVocab/lm/4gram.arpa > DATA/ckptsLargeVocab/lm/G_4_gram.fst.txt\n",
    "\n",
    "3) python prepare_lang_fst.py   --lang-dir DATA/ckptsLargeVocab/lang_phone   --has-silence 1   --ngram-G DATA/ckptsLargeVocab/lm/G_4_gram.fst.txt\n",
    "4) python compile_hlg.py   --lang-dir DATA/ckptsLargeVocab/lang_phone   --lm G_4_gram\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "langDir = \"/mnt/dataDrive/emgFullCorpora/toUpload/DATA/ckptsLargeVocab/lang_phone\"\n",
    "device = \"cpu\"  \n",
    "\n",
    "\n",
    "def readSymbolTable(path):\n",
    "    s2i, i2s, maxId = {}, {}, -1\n",
    "    with open(path, \"r\", encoding = \"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            s, i = line.split()\n",
    "            i = int(i)\n",
    "            s2i[s] = i\n",
    "            i2s[i] = s\n",
    "            if i > maxId:\n",
    "                maxId = i\n",
    "    return s2i, i2s, maxId + 1 \n",
    "\n",
    "def safeLoadFsaDict(ptPath):\n",
    "    try:\n",
    "        d = torch.load(ptPath, map_location = \"cpu\", weights_only = True)\n",
    "    except TypeError:\n",
    "        d = torch.load(ptPath, map_location = \"cpu\")\n",
    "    except Exception:\n",
    "        d = torch.load(ptPath, map_location = \"cpu\", weights_only = False)\n",
    "    return d\n",
    "\n",
    "def buildAcousticTokenIds(tok2id, VMinus1):\n",
    "    EXCLUDE = {\n",
    "        \"<eps>\", \"<blk>\",\n",
    "        \"SPN\", \"NSN\", \"<SPOKEN_NOISE>\", \"<UNK>\", \"<NOISE>\", \"<SIL>\", \"<NOISE_SIG>\"\n",
    "    }\n",
    "    items = [(sym, tid) for sym, tid in tok2id.items()\n",
    "             if sym not in EXCLUDE and not sym.startswith(\"#\") and sym != \"<eps>\"]\n",
    "\n",
    "    items.sort(key = lambda x: x[1])\n",
    "\n",
    "    if len(items) < VMinus1:\n",
    "        found = \", \".join(sym for sym, _ in items)\n",
    "        raise RuntimeError(\n",
    "            f\"Found only {len(items)} acoustic tokens, need {VMinus1}.\\n\"\n",
    "            f\"Found: {found}\\n\"\n",
    "            f\"Tip: If your training used a different phone inventory, add those symbols back.\"\n",
    "        )\n",
    "    items = items[:VMinus1]\n",
    "    return [tid for _, tid in items]\n",
    "\n",
    "\n",
    "\n",
    "langPath = Path(langDir)\n",
    "assert langPath.exists(), f\"Not found: {langPath}\"\n",
    "\n",
    "tok2id, id2tok, CTok = readSymbolTable(langPath / \"tokens.txt\")\n",
    "w2id, id2word, CWord = readSymbolTable(langPath / \"words.txt\")\n",
    "\n",
    "lex = Lexicon(langDir)\n",
    "\n",
    "HLGDict = safeLoadFsaDict(langPath / \"HLG.pt\")\n",
    "HLG = k2.Fsa.from_dict(HLGDict)\n",
    "HLG = k2.arc_sort(HLG).to(device).requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODED_WORDS = []\n",
    "for i in range(len(outs)):\n",
    "    print(i)\n",
    "    logitsORlogprobsTV = torch.as_tensor(outs[i][:int(testInputLengths[i]), :], dtype = torch.float32)\n",
    "    assert logitsORlogprobsTV.ndim == 2, logitsORlogprobsTV.shape\n",
    "    T, V = logitsORlogprobsTV.shape\n",
    "    assert V >= 2, \"Expect at least 2 classes (phones + blank).\"\n",
    "    BLANK_CLASS = V - 1  \n",
    "\n",
    "    nnetOutput = logitsORlogprobsTV.unsqueeze(0).to(device)\n",
    "\n",
    "    acousticTokenIds = buildAcousticTokenIds(tok2id, VMinus1 = V-1)\n",
    "    assert len(acousticTokenIds) == V-1\n",
    "\n",
    "    \n",
    "    NEG_INF = -1e30\n",
    "    N = nnetOutput.shape[0]\n",
    "    nnetLogitsToken = torch.full(\n",
    "        (N, T, CTok), NEG_INF, dtype = nnetOutput.dtype, device = nnetOutput.device\n",
    "    )\n",
    "\n",
    "    nnetLogitsToken[..., 0] = nnetOutput[..., BLANK_CLASS]\n",
    "\n",
    "    for clsIdx, tokId in enumerate(acousticTokenIds):\n",
    "        nnetLogitsToken[..., tokId] = nnetOutput[..., clsIdx]\n",
    "\n",
    "    nnetLogprobsToken = nnetLogitsToken\n",
    "    supervisionSegments = torch.tensor([[0, 0, T]], dtype = torch.int32, device = \"cpu\")\n",
    "    HLG = HLG.to(nnetLogprobsToken.device)\n",
    "    \n",
    "    lattice = get_lattice(\n",
    "    nnet_output = nnetLogprobsToken,\n",
    "    decoding_graph = HLG,\n",
    "    supervision_segments = supervisionSegments,\n",
    "    search_beam = 50,\n",
    "    output_beam = 50,\n",
    "    min_active_states = 30,\n",
    "    max_active_states = 10000)\n",
    "\n",
    "    best = one_best_decoding(lattice, use_double_scores = True)\n",
    "    idsList = get_texts(best) \n",
    "    decodedWords = [id2word[i] for i in idsList[0] if i in id2word]\n",
    "    DECODED_WORDS.append(decodedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "WER = []\n",
    "for i in range(len(DECODED_WORDS)):\n",
    "    ref = LABELS[9000 + i].lower()\n",
    "    hyp = \" \".join(DECODED_WORDS[i]).lower()\n",
    "\n",
    "    werScore = jiwer.wer(ref, hyp)\n",
    "    WER.append(werScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7183350250989845\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(WER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 991 1346 1756  452 1056  682  989 1619   59 1476 1478  214 1920  997\n",
      " 1751  170  564 1179 1381  835  939  687 1373 1222 1035 1007 1538 1157\n",
      "  852 1467  173  839  536 1623 1326 1504 1214  784 1491 1341 1690  995\n",
      " 1821 1807  973  621  620   34 1855  983  850  633  520   90 1732  985\n",
      " 1121  741    8   48  215  594  655 1318 1315  690  669  134  540 1620\n",
      " 1802   35  627 1185  120 1072 1771 1898  815  392 1714 1542 1135  674\n",
      "  108 1924 1581  759  362 1447  169 1543  797  821 1927 1216 1518 1800\n",
      "  703 1436]\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theyve got a nation\n",
      "theyve got a nice one\n"
     ]
    }
   ],
   "source": [
    "which = 991\n",
    "print(\" \".join(DECODED_WORDS[which]).lower())\n",
    "print(LABELS[9000 + which])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
